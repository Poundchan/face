<!DOCTYPE html>
<html>
<head>
    <script src="face-api.js"></script>
    <script src="js/commons.js"></script>
    <script src="js/faceDetectionControls.js"></script>
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/materialize/0.100.2/css/materialize.css">
    <script type="text/javascript" src="https://code.jquery.com/jquery-2.1.1.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/materialize/0.100.2/js/materialize.min.js"></script>
</head>
<body>
<div id="bsm-container">

    <div class="progress" id="loader">
        <div class="indeterminate"></div>
    </div>
    <div id="bsm-video">
        <video onloadedmetadata="onPlay(this)" id="inputVideo" autoplay muted playsinline></video>
        <canvas id="overlay"/>
    </div>
</div>


<!-- ssd_mobilenetv1_controls -->
<!-- ssd_mobilenetv1_controls -->

<!-- tiny_face_detector_controls -->
<!-- tiny_face_detector_controls -->

<!-- mtcnn_controls -->
<!-- mtcnn_controls -->

</body>

<script>
	let forwardTimes = []
	let predictedAges = []
	let withBoxes = true

	function onChangeHideBoundingBoxes(e) {
		withBoxes = !$(e.target).prop('checked')
	}

	function updateTimeStats(timeInMs) {
		forwardTimes = [timeInMs].concat(forwardTimes).slice(0, 30)
		const avgTimeInMs = forwardTimes.reduce((total, t) => total + t) / forwardTimes.length
		$('#time').val(`${Math.round(avgTimeInMs)} ms`)
		$('#fps').val(`${faceapi.round(1000 / avgTimeInMs)}`)
	}

	function interpolateAgePredictions(age) {
		predictedAges = [age].concat(predictedAges).slice(0, 30)
		const avgPredictedAge = predictedAges.reduce((total, a) => total + a) / predictedAges.length
		return avgPredictedAge
	}

	async function onPlay() {
		const videoEl = $('#inputVideo').get(0)

		if (videoEl.paused || videoEl.ended || !isFaceDetectionModelLoaded())
			return setTimeout(() => onPlay())

		const options = getFaceDetectorOptions()

		const ts = Date.now()

		const result = await faceapi.detectAllFaces(videoEl, options)
		console.log('result',result)

		updateTimeStats(Date.now() - ts)
		const displaySize = { width: 925, height: 520 }
		// resize the overlay canvas to the input dimensions
		const canvas = document.getElementById('overlay')
		faceapi.matchDimensions(canvas, displaySize)
        if(result){
			const detectionsWithExpressions = await faceapi
				.detectAllFaces(videoEl)
				.withFaceLandmarks()
				.withFaceExpressions()
			// resize the detected boxes and landmarks in case your displayed image has a different size than the original
			const resizedResults = faceapi.resizeResults(detectionsWithExpressions, displaySize)
			// draw detections into the canvas
			faceapi.draw.drawDetections(canvas, resizedResults)
			// draw a textbox displaying the face expressions with minimum probability into the canvas
			const minProbability = 0.05
			faceapi.draw.drawFaceExpressions(canvas, resizedResults, minProbability)

		}
		// if (result) {
		// 	const canvas = $('#overlay').get(0)
		// 	const dims = faceapi.matchDimensions(canvas, videoEl, true)
        //
		// 	const resizedResult = faceapi.resizeResults(result, dims)
		// 	if (withBoxes) {
		// 		faceapi.draw.drawDetections(canvas, resizedResult)
		// 	}
		// 	const {age, gender, genderProbability} = resizedResult
        //
		// 	// interpolate gender predictions over last 30 frames
		// 	// to make the displayed age more stable
		// 	const interpolatedAge = interpolateAgePredictions(age)
		// 	new faceapi.draw.DrawTextField(
		// 		[
		// 			`${faceapi.round(interpolatedAge, 0)} years`,
		// 			`${gender} (${faceapi.round(genderProbability)})`
		// 		],
		// 		result.detection.box.bottomLeft
		// 	).draw(canvas)
		// }

		setTimeout(() => onPlay())
	}

	async function run() {
		// load face detection and face expression recognition models
		await changeFaceDetector(TINY_FACE_DETECTOR)
		await faceapi.loadFaceLandmarkModel('/')
		await faceapi.loadFaceExpressionModel('/')
		await faceapi.nets.ageGenderNet.load('/')
		changeInputSize(224)

		// try to access users webcam and stream the images
		// to the video element
		const stream = await navigator.mediaDevices.getUserMedia({video: {width: 925, height: 520 }})
		const videoEl = $('#inputVideo').get(0)
		videoEl.srcObject = stream
	}

	function updateResults() {
	}

	$(document).ready(function () {
		initFaceDetectionControls()
		run()
	})
</script>
</body>
</html>
